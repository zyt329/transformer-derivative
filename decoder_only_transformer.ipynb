{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['72exp^(9x)', '340exp^(17x)+18exp^(x)', '144exp^(12x)+180exp^(20x)', '26x+36x+100x^4+114x^5', '456exp^(19x)*x+24exp^(19x)']\n"
     ]
    }
   ],
   "source": [
    "# given original or target expression, return splitted\n",
    "def split_exprs(input, output):\n",
    "    input = input.strip()\n",
    "    input, var = input.split(\")/d\")\n",
    "\n",
    "    # exprs.append(expr[2:].replace(var, 'x'))\n",
    "    \n",
    "    input = input[2:]\n",
    "    # Step 1: Replace `exp`, `cos`, and `sin` with placeholders\n",
    "    input = input.replace('exp', 'E')\n",
    "    input = input.replace('cos', 'C')\n",
    "    input = input.replace('sin', 'S')\n",
    "    output = output.replace('exp', 'E')\n",
    "    output = output.replace('cos', 'C')\n",
    "    output = output.replace('sin', 'S')\n",
    "\n",
    "    # Step 2: Substitute `e`, `s`, `c` with 'x' (only where theyâ€™re standalone)\n",
    "    input = input.replace(var, 'x')\n",
    "    output = output.replace(var, 'x')\n",
    "\n",
    "    # Step 3: Restore the placeholders back to their original words\n",
    "    input = input.replace('E', 'exp')\n",
    "    input = input.replace('C', 'cos')\n",
    "    input = input.replace('S', 'sin')\n",
    "    output = output.replace('E', 'exp')\n",
    "    output = output.replace('C', 'cos')\n",
    "    output = output.replace('S', 'sin')\n",
    "\n",
    "    return input, output\n",
    "\n",
    "# read training data line by line\n",
    "def read_data(file):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    with open(file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            \n",
    "            input, output = line.split('=')\n",
    "            input = input.strip()\n",
    "\n",
    "            # split original and target expression\n",
    "            input, output = split_exprs(input, output)\n",
    "            \n",
    "            inputs.append(input)\n",
    "            outputs.append(output)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "print(read_data(\"train.txt\")[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', 'exp^(', '9', 'x', ')', '=', '7', '2', 'exp^(', '9', 'x', ')'], ['2', '0', 'exp^(', '1', '7', 'x', ')', '+', '1', '8', 'exp^(', 'x', ')', '=', '3', '4', '0', 'exp^(', '1', '7', 'x', ')', '+', '1', '8', 'exp^(', 'x', ')'], ['1', '2', 'exp^(', '1', '2', 'x', ')', '+', '9', 'exp^(', '2', '0', 'x', ')', '=', '1', '4', '4', 'exp^(', '1', '2', 'x', ')', '+', '1', '8', '0', 'exp^(', '2', '0', 'x', ')'], ['1', '3', 'x', '^', '2', '+', '1', '8', 'x', '^', '2', '+', '2', '0', 'x', '^', '5', '+', '1', '9', 'x', '^', '6', '=', '2', '6', 'x', '+', '3', '6', 'x', '+', '1', '0', '0', 'x', '^', '4', '+', '1', '1', '4', 'x', '^', '5'], ['2', '4', 'exp^(', '1', '9', 'x', ')', '*', 'x', '=', '4', '5', '6', 'exp^(', '1', '9', 'x', ')', '*', 'x', '+', '2', '4', 'exp^(', '1', '9', 'x', ')']]\n",
      "[['exp^(', '9', 'x', ')', '=', '7', '2', 'exp^(', '9', 'x', ')', '<eos>'], ['0', 'exp^(', '1', '7', 'x', ')', '+', '1', '8', 'exp^(', 'x', ')', '=', '3', '4', '0', 'exp^(', '1', '7', 'x', ')', '+', '1', '8', 'exp^(', 'x', ')', '<eos>'], ['2', 'exp^(', '1', '2', 'x', ')', '+', '9', 'exp^(', '2', '0', 'x', ')', '=', '1', '4', '4', 'exp^(', '1', '2', 'x', ')', '+', '1', '8', '0', 'exp^(', '2', '0', 'x', ')', '<eos>'], ['3', 'x', '^', '2', '+', '1', '8', 'x', '^', '2', '+', '2', '0', 'x', '^', '5', '+', '1', '9', 'x', '^', '6', '=', '2', '6', 'x', '+', '3', '6', 'x', '+', '1', '0', '0', 'x', '^', '4', '+', '1', '1', '4', 'x', '^', '5', '<eos>'], ['4', 'exp^(', '1', '9', 'x', ')', '*', 'x', '=', '4', '5', '6', 'exp^(', '1', '9', 'x', ')', '*', 'x', '+', '2', '4', 'exp^(', '1', '9', 'x', ')', '<eos>']]\n",
      "tensor([ 5, 13, 14, 23,  9])\n"
     ]
    }
   ],
   "source": [
    "# break up the sequence to list of tokens\n",
    "def add_space(expr):\n",
    "    regex = '(exp\\^\\(|sin\\(|cos\\(|sin\\^|cos\\^|\\+|\\-|\\(|\\)|\\^|\\*|x|\\d)'\n",
    "    splitted = re.split(regex, expr)\n",
    "    splitted = [x for x in splitted if x is not None and x != '']\n",
    "    return splitted\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "inputs, outputs = read_data(\"train.txt\")\n",
    "X = [add_space(expr) for expr in inputs]\n",
    "y = [add_space(expr) for expr in outputs]\n",
    "\n",
    "input_lens = torch.tensor([len(x) for x in X], dtype=torch.long) #debugging\n",
    "input = [a+[\"=\"]+b for a,b in zip(X, y)]\n",
    "output = [expr[1:]+[\"<eos>\"] for expr in input]\n",
    "print(input[:5])\n",
    "print(output[:5])\n",
    "print(input_lens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building vocab, converting to encoded inputs and outputs\n",
      "done converting input and output to encoded\n",
      "Vocabulary: {'^': 4, '[UNK]': 0, 's': 34, '3': 14, '[PAD]': 1, '8': 13, 'x': 2, 'i': 33, ')': 5, '1': 3, 'r': 25, '2': 6, 'o': 37, 'exp^(': 7, 'p': 29, '0': 8, '*': 9, 'k': 35, '+': 10, 't': 31, '4': 11, 'v': 41, '6': 12, 'u': 26, '5': 15, '=': 16, 'w': 39, '7': 17, 'y': 28, '9': 18, '(': 19, 'sin^': 20, 'cos^': 21, 'm': 42, '-': 22, 'cos(': 23, 'sin(': 24, 'b': 27, 'z': 30, 'a': 32, 'n': 36, 'c': 38, 'e': 40}\n",
      "Encoded Inputs: tensor([[13,  7, 18,  2,  5, 16, 17,  6,  7, 18,  2,  5,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 6,  8,  7,  3, 17,  2,  5, 10,  3, 13,  7,  2,  5, 16, 14, 11,  8,  7,\n",
      "          3, 17,  2,  5, 10,  3, 13,  7,  2,  5,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 3,  6,  7,  3,  6,  2,  5, 10, 18,  7,  6,  8,  2,  5, 16,  3, 11, 11,\n",
      "          7,  3,  6,  2,  5, 10,  3, 13,  8,  7,  6,  8,  2,  5,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 3, 14,  2,  4,  6, 10,  3, 13,  2,  4,  6, 10,  6,  8,  2,  4, 15, 10,\n",
      "          3, 18,  2,  4, 12, 16,  6, 12,  2, 10, 14, 12,  2, 10,  3,  8,  8,  2,\n",
      "          4, 11, 10,  3,  3, 11,  2,  4, 15,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 6, 11,  7,  3, 18,  2,  5,  9,  2, 16, 11, 15, 12,  7,  3, 18,  2,  5,\n",
      "          9,  2, 10,  6, 11,  7,  3, 18,  2,  5,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
      "Encoded Outputs: tensor([[ 7, 18,  2,  5, 16, 17,  6,  7, 18,  2,  5,  0,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 8,  7,  3, 17,  2,  5, 10,  3, 13,  7,  2,  5, 16, 14, 11,  8,  7,  3,\n",
      "         17,  2,  5, 10,  3, 13,  7,  2,  5,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 6,  7,  3,  6,  2,  5, 10, 18,  7,  6,  8,  2,  5, 16,  3, 11, 11,  7,\n",
      "          3,  6,  2,  5, 10,  3, 13,  8,  7,  6,  8,  2,  5,  0,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [14,  2,  4,  6, 10,  3, 13,  2,  4,  6, 10,  6,  8,  2,  4, 15, 10,  3,\n",
      "         18,  2,  4, 12, 16,  6, 12,  2, 10, 14, 12,  2, 10,  3,  8,  8,  2,  4,\n",
      "         11, 10,  3,  3, 11,  2,  4, 15,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [11,  7,  3, 18,  2,  5,  9,  2, 16, 11, 15, 12,  7,  3, 18,  2,  5,  9,\n",
      "          2, 10,  6, 11,  7,  3, 18,  2,  5,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "vocab = build_vocab_from_iterator(input, specials=[\"[UNK]\", \"[PAD]\"])\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])  # Set default index for OOV tokens\n",
    "\n",
    "print(\"done building vocab, converting to encoded inputs and outputs\")\n",
    "\n",
    "input = [torch.tensor(vocab(tokens), dtype=torch.long) for tokens in input]\n",
    "output = [torch.tensor(vocab(tokens), dtype=torch.long) for tokens in output]\n",
    "# y = torch.tensor([torch.tensor(vocab(tokens), dtype=torch.long) for tokens in y])\n",
    "print(\"done converting input and output to encoded\")\n",
    "\n",
    "\n",
    "# Pad the sequences to make them equal length\n",
    "input = pad_sequence(input, batch_first=True, padding_value=vocab[\"[PAD]\"])\n",
    "output = pad_sequence(output, batch_first=True, padding_value=vocab[\"[PAD]\"])\n",
    "seq_len = input.size(1)\n",
    "\n",
    "# Print vocabulary and integer encoding\n",
    "print(\"Vocabulary:\", vocab.get_stoi())      # stoi: string-to-integer mapping\n",
    "print(\"Encoded Inputs:\", input[:5])  # Convert tokens to integer IDs\n",
    "print(\"Encoded Outputs:\", output[:5])  # Convert tokens to integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Determine the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def get_angs(pos, i, d_model):\n",
    "    return pos / np.power(10000, (2*(i//2) / d_model))\n",
    "\n",
    "def pos_encodings(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Should return a positional encoding tensor of shape (1, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    angs = get_angs(np.arange(seq_len)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angs[:, 0::2] = np.sin(angs[:, 0::2])\n",
    "    angs[:, 1::2] = np.cos(angs[:, 1::2])\n",
    "    return torch.tensor(angs[None, :, :], dtype=torch.float32)\n",
    "\n",
    "# test\n",
    "# print(pos_encodings(32).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example mask for first sequence (input_len=2, padding at 4,5):\n",
      "[[[[0 1 1 ... 1 1 1]\n",
      "   [0 0 1 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   ...\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]]]\n",
      "\n",
      "\n",
      " [[[0 1 1 ... 1 1 1]\n",
      "   [0 0 1 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   ...\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]\n",
      "   [0 0 0 ... 1 1 1]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nShould print something like:\\n[[0 0 1 1 1 1]  # First input token: can attend to input only\\n [0 0 1 1 1 1]  # Second input token: can attend to input only\\n [0 0 0 1 1 1]  # First output token: can attend to previous, not future/padding\\n [0 0 0 0 1 1]  # Second output token: can attend to previous, not future/padding\\n [1 1 1 1 1 1]  # PAD token: can't attend to anything\\n [1 1 1 1 1 1]] # PAD token: can't attend to anything\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding mask (batch_size,1,1,seq_len) applied to (batch_size, num_head, seq_len, seq_len)\n",
    "def create_padding_mask(seq):\n",
    "    return (seq == vocab['[PAD]'])[:, None, None, :]\n",
    "\n",
    "def create_combined_mask(seq_len, input_lens, padding_mask):\n",
    "    \"\"\"\n",
    "    seq_len: length of sequence\n",
    "    input_lens: tensor of shape (batch_size,) with length of input portion\n",
    "    padding_mask: tensor of shape (batch_size, seq_len) where True indicates padding\n",
    "    \n",
    "    Returns: mask of shape (batch_size, seq_len, seq_len) where True indicates masked positions\n",
    "    \"\"\"\n",
    "    batch_size = len(input_lens)\n",
    "    # device = input_lens.device\n",
    "    \n",
    "    # Create position indices\n",
    "    pos_i = torch.arange(seq_len, device=device)[None, :, None]  # (1, seq_len, 1)\n",
    "    pos_j = torch.arange(seq_len, device=device)[None, None, :]  # (1, 1, seq_len)\n",
    "    input_lens = input_lens[:, None, None]  # (batch_size, 1, 1)\n",
    "    \n",
    "    # 1. Create pattern mask (input attention and causal output attention)\n",
    "    # For input positions: can only attend to input positions\n",
    "    # is_input_query = pos_i < input_lens\n",
    "    # is_output_key = pos_j >= input_lens\n",
    "    # cross_mask = is_input_query & is_output_key\n",
    "    \n",
    "    # # For output positions: can attend to all previous positions (causal mask)\n",
    "    # is_output_query = pos_i >= input_lens\n",
    "    is_future = pos_i < pos_j\n",
    "    look_ahead_mask = is_future #is_output_query & is_future\n",
    "    \n",
    "    pattern_mask = look_ahead_mask# cross_mask | look_ahead_mask  # (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # 2. Create padding mask\n",
    "    # Key padding mask: mask padded keys for all queries\n",
    "    padding_mask = padding_mask.squeeze()\n",
    "    key_padding = padding_mask[:, None, :]  # (batch_size, 1, seq_len)\n",
    "    key_padding = key_padding.expand(-1, seq_len, -1)  # (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Query padding mask: mask padded queries for all keys (not used)\n",
    "    # query_padding = padding_mask[:, :, None]  # (batch_size, seq_len, 1)\n",
    "    # query_padding = query_padding.expand(-1, -1, seq_len)  # (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # 3. Combine all masks\n",
    "    final_mask = pattern_mask | key_padding #| query_padding\n",
    "    \n",
    "    return final_mask[:, None, :, :]  # (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 2\n",
    "\n",
    "# Example sequence lengths\n",
    "input_len = torch.tensor(input_lens[:batch_size])  # First sample: 2 input tokens, Second sample: 3 input tokens\n",
    "\n",
    "# Example padding mask (1 indicates padding)\n",
    "# padding_mask = torch.zeros((batch_size, seq_len), dtype=torch.bool)\n",
    "# padding_mask[0, 4:] = True  # First sample has padding at positions 4,5\n",
    "# padding_mask[1, 5:] = True  # Second sample has padding at position 5\n",
    "input_test = input[:batch_size]\n",
    "padding_mask = create_padding_mask(input_test)\n",
    "\n",
    "mask = create_combined_mask(seq_len, input_len, padding_mask)\n",
    "\n",
    "# Print example for first sample\n",
    "print(\"Example mask for first sequence (input_len=2, padding at 4,5):\")\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "print(mask.int().numpy())\n",
    "\"\"\"\n",
    "Should print something like:\n",
    "[[0 0 1 1 1 1]  # First input token: can attend to input only\n",
    " [0 0 1 1 1 1]  # Second input token: can attend to input only\n",
    " [0 0 0 1 1 1]  # First output token: can attend to previous, not future/padding\n",
    " [0 0 0 0 1 1]  # Second output token: can attend to previous, not future/padding\n",
    " [1 1 1 1 1 1]  # PAD token: can't attend to anything\n",
    " [1 1 1 1 1 1]] # PAD token: can't attend to anything\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example output mask for first sequence (input_len=2, padding at 4,5):\n",
      "[[1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# create masks\n",
    "\n",
    "# padding mask (batch_size,seq_len, 1) applied to (batch_size, seq_len, vocab_len)\n",
    "# input seq of the shape (batch_size, seq_len)\n",
    "# def create_output_padding_mask(seq):\n",
    "#     return (seq == vocab['[PAD]'])[:,:,None]\n",
    "\n",
    "# the output mask, of the shape (batch_size, seq_len, vocab_len)\n",
    "def create_output_mask(seq, input_lens, padding_mask):\n",
    "    batch_size, seq_len = seq.shape\n",
    "    \n",
    "    # Create position indices tensor\n",
    "    positions = torch.arange(seq_len, device=device)[None, :]  # (1, seq_len)\n",
    "    positions = positions.expand(batch_size, -1)  # (batch_size, seq_len)\n",
    "    \n",
    "    # Create mask using input lengths\n",
    "    input_lens = torch.tensor(input_lens, device=seq.device)[:, None]  # (batch_size, 1)\n",
    "    mask = positions < input_lens  # True for positions after input length\n",
    "\n",
    "    # Add padding mask\n",
    "    mask = mask | padding_mask.squeeze()  # Combine with padding mask\n",
    "    \n",
    "    # Add broadcasting dimensions for attention\n",
    "    return mask[:,:]  # (batch_size, seq_len, vocab_len)\n",
    "\n",
    "# Example usage:\n",
    "padding_mask_test = create_padding_mask(input_test)\n",
    "mask_test = create_output_mask(input_test, input_lens[:batch_size], padding_mask_test)\n",
    "\n",
    "print(\"Example output mask for first sequence (input_len=2, padding at 4,5):\")\n",
    "print(mask_test.squeeze().int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mechanism\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # dimension of each head as well as \n",
    "        # dq, dk, dv\n",
    "        self.d_head = d_model//num_heads \n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, batch_size, x):\n",
    "        x = x.view(batch_size, self.num_heads, -1, self.d_head)\n",
    "        # (batch_size, num_head, seq_len, d_head)\n",
    "        return x \n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        # (batch_size, seq_len, d_model)\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "        q = self.split_heads(batch_size, q)\n",
    "        k = self.split_heads(batch_size, k)\n",
    "        v = self.split_heads(batch_size, v)\n",
    "        # (batch_size, num_head, seq_len, d_head)\n",
    "        \n",
    "        attention = torch.einsum('ijml,ijnl->ijmn',q,k) / self.d_head**0.5\n",
    "        # (batch_size, num_head, seq_lenq, seq_lenk)\n",
    "\n",
    "        if mask is not None:\n",
    "            # print(\"unmasked attention:\")\n",
    "            # print(attention[:,1,:,:])\n",
    "            # print(\"==\"*100)\n",
    "            attention = attention.masked_fill(mask == 1, float('-inf'))\n",
    "            # print(\"masked attention:\")\n",
    "            # for i in range(attention.shape[2]):\n",
    "            #     print(attention[:,1,i,:])\n",
    "            # print(\"==\"*100)\n",
    "\n",
    "\n",
    "        # apply softmax to the dimension corresponding to k\n",
    "        attention = torch.nn.functional.softmax(attention, dim = -1)\n",
    "        # print(\"softmax attention:\")\n",
    "        # for i in range(attention.shape[2]):\n",
    "        #     print(attention[:,1,i,:])\n",
    "        # print(\"==\"*100)\n",
    "        \n",
    "\n",
    "        out = torch.einsum('ijkl,ijlq->ijkq',attention, v)\n",
    "        # (batch_size, num_head, seq_lenq, d_head)\n",
    "        out = out.view(batch_size, -1, self.d_model)\n",
    "        # (batch_size, seq_lenq, d_model)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# MultiHeadAttention(32,4).forward(torch.randn(2, 10, 32), None).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.LN1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model)\n",
    "        )\n",
    "        self.LN2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        out1 = self.mha(x,mask)\n",
    "        out1 = self.dropout1(out1)\n",
    "        out1 = self.LN1(x + out1)\n",
    "\n",
    "        out2 = self.ffn(out1)\n",
    "        out2 = self.dropout2(out2)\n",
    "        out2 = self.LN2(out1 + out2)\n",
    "\n",
    "        return out2 # (batch_size, seq_len, d_model)\n",
    "\n",
    "# print(DecoderLayer(32,4,0.1)(torch.randn(2, 10, 32), None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_len, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_len, d_model)\n",
    "\n",
    "        self.decoderlayers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dense = nn.Linear(d_model, vocab_len)\n",
    "\n",
    "    def create_mask(self, x):\n",
    "        mask = (x == 0).unsqueeze(1).unsqueeze(2)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)\n",
    "        x += pos_encodings(x.shape[1], self.d_model)\n",
    "\n",
    "        # print(\"Embedding stats:\", \n",
    "        #   f\"mean={x.mean().item():.4f}, \"\n",
    "        #   f\"max={x.max().item():.4f}\")\n",
    "        for i, decoderlayer in enumerate(self.decoderlayers):\n",
    "            x = decoderlayer(x, mask)\n",
    "            # print(f\"Layer {i} output:\", \n",
    "            #   f\"mean={x.mean().item():.4f}, \"\n",
    "            #   f\"max={x.max().item():.4f}\")\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# vocab_len_test = 100\n",
    "# batch_size_test = 64\n",
    "# seq_len_test = 20\n",
    "# print(Decoder(vocab_len_test, 6, 32, 4, 0.1)(torch.randint(0, vocab_len_test, (batch_size_test, seq_len_test)) , None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating masks, preparing data...\n",
      "Done preparing data\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create masks\n",
    "print(\"Creating masks...\")\n",
    "padding_mask = create_padding_mask(input)\n",
    "output_mask = create_output_mask(input, input_lens, padding_mask)\n",
    "combined_mask = create_combined_mask(seq_len, input_lens, padding_mask)\n",
    "print(\"Done creating masks, preparing data...\")\n",
    "\n",
    "# train, dev, test split\n",
    "X_train, X_test, y_train, y_test, output_mask_train, output_mask_test, combined_mask_train, combined_mask_test = train_test_split(input, output, output_mask, combined_mask, test_size=0.05, random_state=1)\n",
    "X_train, X_dev, y_train, y_dev, output_mask_train, output_mask_dev, combined_mask_train, combined_mask_dev = train_test_split(X_train, y_train, output_mask_train, combined_mask_train, test_size=0.05, random_state=2)\n",
    "\n",
    "\n",
    "# prepare data\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train, combined_mask_train, output_mask_train), batch_size=32, shuffle=False)\n",
    "dev_loader = DataLoader(TensorDataset(X_dev, y_dev, combined_mask_dev, output_mask_dev), batch_size=X_dev.size(0), shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test, combined_mask_test, output_mask_test), batch_size=X_test.size(0), shuffle=False)\n",
    "\n",
    "# dev_loader = DataLoader(TensorDataset(X_dev, combined_mask_dev, output_mask_dev), batch_size=X_dev.size(0), shuffle=False)\n",
    "# test_loader = DataLoader(TensorDataset(X_test, combined_mask_test, output_mask_test), batch_size=X_test.size(0), shuffle=False)\n",
    "print(\"Done preparing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.0728)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss calculation\n",
    "def compute_masked_loss(preds, output, output_mask):\n",
    "    \n",
    "    # Calculate per-position loss\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    # preds from: (batch_size, seq_len, vocab_size)\n",
    "    #         to: (batch_size * seq_len, vocab_size)\n",
    "    # output from: (batch_size, seq_len)\n",
    "    #         to: (batch_size * seq_len)\n",
    "    loss = loss_fn(preds.view(-1, preds.size(-1)), \n",
    "                  output.view(-1))\n",
    "    # # Debug prints\n",
    "    # if torch.isnan(loss).any():\n",
    "    #     print(\"NaN in per-position loss!\")\n",
    "    #     print(\"Max logit value:\", torch.max(preds))\n",
    "    #     print(\"Min logit value:\", torch.min(preds))\n",
    "    #     print(\"Any inf in logits:\", torch.isinf(preds).any())\n",
    "\n",
    "    # loss shape from: (batch_size * seq_len)\n",
    "    #              to: (batch_size, seq_len)\n",
    "    loss = loss.view(output.shape)\n",
    "    \n",
    "    # Inverse mask for loss calculation (1 - mask gives 1 (True) for positions we want to mask out)\n",
    "    loss = loss * ~output_mask\n",
    "    final_loss = loss.sum() / (~output_mask).sum()\n",
    "\n",
    "    if torch.isnan(final_loss):\n",
    "        print(\"NaN in final loss!\")\n",
    "        print(\"Sum of losses:\", loss.sum())\n",
    "        print(\"Number of valid positions:\", (~output_mask).sum())\n",
    "    \n",
    "    # Average loss over non-masked positions\n",
    "    return final_loss\n",
    "\n",
    "def compute_masked_precision(preds, output, output_mask):\n",
    "    # Get predictions with highest probability\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    \n",
    "    # Count correct predictions\n",
    "    correct = (preds == output) & ~output_mask\n",
    "    \n",
    "    # Calculate precision\n",
    "    return correct.sum().float() / (~output_mask).sum()\n",
    "\n",
    "\n",
    "# test\n",
    "preds = torch.randn(64, 20, 100)\n",
    "input_test = torch.randint(0, 100, (64, 20))\n",
    "output_mask_test = create_output_mask(input_test, input_lens[:64], create_padding_mask(input_test))\n",
    "compute_masked_loss(preds, input_test, output_mask_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/28204, Loss: 3.8977017402648926, running loss: 0.1948850870132448, Validation Loss: 3.876202344894409, Validation Precision: 0.033285290002822876\n",
      "Epoch 1/10, Batch 1251/28204, Loss: 2.904059648513794, running loss: 2.875323553052806, Validation Loss: 2.842707633972168, Validation Precision: 0.2787513732910156\n",
      "Epoch 1/10, Batch 2501/28204, Loss: 2.527920722961426, running loss: 2.5524608274797025, Validation Loss: 2.5250394344329834, Validation Precision: 0.30743902921676636\n",
      "Epoch 1/10, Batch 3751/28204, Loss: 2.3542659282684326, running loss: 2.3735301598173173, Validation Loss: 2.3326737880706787, Validation Precision: 0.324112206697464\n",
      "Epoch 1/10, Batch 5001/28204, Loss: 2.219581127166748, running loss: 2.21100564833224, Validation Loss: 2.1914734840393066, Validation Precision: 0.3495895266532898\n",
      "Epoch 1/10, Batch 6251/28204, Loss: 2.1205790042877197, running loss: 2.114795488761512, Validation Loss: 2.0874600410461426, Validation Precision: 0.37365368008613586\n",
      "Epoch 1/10, Batch 7501/28204, Loss: 2.0301520824432373, running loss: 2.0241853446269467, Validation Loss: 2.007053852081299, Validation Precision: 0.394955039024353\n",
      "Epoch 1/10, Batch 8751/28204, Loss: 2.0191731452941895, running loss: 1.9769343520225362, Validation Loss: 1.940628170967102, Validation Precision: 0.4090557098388672\n",
      "Epoch 1/10, Batch 10001/28204, Loss: 1.835434913635254, running loss: 1.9062363188596096, Validation Loss: 1.8867309093475342, Validation Precision: 0.41800785064697266\n",
      "Epoch 1/10, Batch 11251/28204, Loss: 1.87155020236969, running loss: 1.888145538207732, Validation Loss: 1.8416643142700195, Validation Precision: 0.4259802997112274\n",
      "Epoch 1/10, Batch 12501/28204, Loss: 1.8538113832473755, running loss: 1.8138378682183594, Validation Loss: 1.8026129007339478, Validation Precision: 0.43418532609939575\n",
      "Epoch 1/10, Batch 13751/28204, Loss: 1.862813949584961, running loss: 1.8115679850265367, Validation Loss: 1.7682652473449707, Validation Precision: 0.44155868887901306\n",
      "Epoch 1/10, Batch 15001/28204, Loss: 1.7802581787109375, running loss: 1.7624861650216919, Validation Loss: 1.7371443510055542, Validation Precision: 0.4477444589138031\n",
      "Epoch 1/10, Batch 16251/28204, Loss: 1.6986440420150757, running loss: 1.731936507968765, Validation Loss: 1.7090686559677124, Validation Precision: 0.45397841930389404\n",
      "Epoch 1/10, Batch 17501/28204, Loss: 1.6966747045516968, running loss: 1.7185123307526782, Validation Loss: 1.6843810081481934, Validation Precision: 0.4581942856311798\n",
      "Epoch 1/10, Batch 18751/28204, Loss: 1.6380139589309692, running loss: 1.6870650561975298, Validation Loss: 1.6624804735183716, Validation Precision: 0.4635613262653351\n",
      "Epoch 1/10, Batch 20001/28204, Loss: 1.6565780639648438, running loss: 1.6596573089173896, Validation Loss: 1.642374038696289, Validation Precision: 0.4679780602455139\n",
      "Epoch 1/10, Batch 21251/28204, Loss: 1.638752818107605, running loss: 1.6445552660601483, Validation Loss: 1.6242927312850952, Validation Precision: 0.47142335772514343\n",
      "Epoch 1/10, Batch 22501/28204, Loss: 1.6437547206878662, running loss: 1.6464536675294301, Validation Loss: 1.6078858375549316, Validation Precision: 0.47564393281936646\n",
      "Epoch 1/10, Batch 23751/28204, Loss: 1.6436887979507446, running loss: 1.6321309557200174, Validation Loss: 1.5919499397277832, Validation Precision: 0.47982925176620483\n",
      "Epoch 1/10, Batch 25001/28204, Loss: 1.6694352626800537, running loss: 1.6263067834504372, Validation Loss: 1.5775681734085083, Validation Precision: 0.4834965467453003\n",
      "Epoch 1/10, Batch 26251/28204, Loss: 1.6786235570907593, running loss: 1.6090422370235005, Validation Loss: 1.5636018514633179, Validation Precision: 0.4871826469898224\n",
      "Epoch 1/10, Batch 27501/28204, Loss: 1.5415003299713135, running loss: 1.59071258186193, Validation Loss: 1.5505598783493042, Validation Precision: 0.49066317081451416\n",
      "Epoch 1/10, Loss: 1.5946532731974494, Validation Loss: 1.5505598783493042, Validation Precision: 0.49066317081451416\n",
      "Epoch 2/10, Batch 1/28204, Loss: 1.566334843635559, running loss: 0.07831674218177802, Validation Loss: 1.5433597564697266, Validation Precision: 0.49293145537376404\n",
      "Epoch 2/10, Batch 1251/28204, Loss: 1.6218795776367188, running loss: 1.5862995537099667, Validation Loss: 1.5316163301467896, Validation Precision: 0.496926486492157\n",
      "Epoch 2/10, Batch 2501/28204, Loss: 1.511256456375122, running loss: 1.5558817795703153, Validation Loss: 1.5201369524002075, Validation Precision: 0.5001027584075928\n",
      "Epoch 2/10, Batch 3751/28204, Loss: 1.5031185150146484, running loss: 1.5718289752092063, Validation Loss: 1.50932776927948, Validation Precision: 0.5034188628196716\n",
      "Epoch 2/10, Batch 5001/28204, Loss: 1.6153337955474854, running loss: 1.537747773700581, Validation Loss: 1.4991786479949951, Validation Precision: 0.5059620141983032\n",
      "Epoch 2/10, Batch 6251/28204, Loss: 1.5085822343826294, running loss: 1.5256207487978317, Validation Loss: 1.488803505897522, Validation Precision: 0.5084640383720398\n",
      "Epoch 2/10, Batch 7501/28204, Loss: 1.464307188987732, running loss: 1.5095255877755822, Validation Loss: 1.479440689086914, Validation Precision: 0.5115745663642883\n",
      "Epoch 2/10, Batch 8751/28204, Loss: 1.6021522283554077, running loss: 1.528256377399945, Validation Loss: 1.470089316368103, Validation Precision: 0.5144254565238953\n"
     ]
    }
   ],
   "source": [
    "# def train(model, train_loader, dev_loader, num_epochs, patience, lr):\n",
    "vocab_len = len(vocab)\n",
    "num_layers = 4 \n",
    "d_model = 32 \n",
    "num_heads = 4 \n",
    "dropout_rate = 0.1\n",
    "\n",
    "num_epochs=10\n",
    "patience=2\n",
    "lr=0.00001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = Decoder(vocab_len, num_layers, d_model, num_heads, dropout_rate)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-4)\n",
    "\n",
    "\n",
    "best_precision = 0\n",
    "num_bad_epochs = 0\n",
    "for epoch in range(num_epochs):\n",
    "    lowest_loss = np.inf\n",
    "    running_loss = 0.0\n",
    "    beta = 0.95\n",
    "    for batch_id, (X_batch, y_batch, combined_mask_batch, output_mask_batch) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # batch_index = 0\n",
    "        # if batch_id != batch_index:\n",
    "        #     continue\n",
    "        # print(f\"running Batch {batch_id}\")\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        combined_mask_batch, output_mask_batch = combined_mask_batch.to(device), output_mask_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch, combined_mask_batch)\n",
    "\n",
    "        # inspect y_pred\n",
    "        # print(y_pred.shape) # (batch_size, seq_len, vocab_len)\n",
    "        # for i in range(y_pred.shape[0]):\n",
    "        #     for pos in range(y_pred.shape[1]):\n",
    "        #         for j in range(y_pred.shape[2]):\n",
    "        #             # if see NaN in any entry of y_pred\n",
    "        #             if torch.isnan(y_pred[i][pos][j]):\n",
    "        #                 pass\n",
    "        #                 print(f\"Encountered NaN at item {i}, position {pos}, vocab_pos {j}\")\n",
    "                        \n",
    "        # apply mask to the output\n",
    "        loss = compute_masked_loss(y_pred, y_batch, output_mask_batch)\n",
    "\n",
    "        # Check for NaNs in loss\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN detected in loss at epoch {epoch}, batch {batch_id}\")\n",
    "            # stop training\n",
    "            raise ValueError(\"NaN detected in loss\")\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Check for exploding gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradients of {name} at epoch {epoch}, batch {batch_id}\")\n",
    "                raise ValueError(\"NaN detected in gradients\")\n",
    "            if param.grad is not None and torch.isinf(param.grad).any():\n",
    "                print(f\"Inf detected in gradients of {name} at epoch {epoch}, batch {batch_id}\")\n",
    "                raise ValueError(\"Inf detected in gradients\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = beta * running_loss + (1 - beta) * loss.item()\n",
    "\n",
    "        if batch_id % 1250 == 0:\n",
    "            # Validation loss\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for (X_dev, y_dev, combined_mask_dev, output_mask_dev) in dev_loader:\n",
    "                    X_dev = X_dev.to(device)\n",
    "                    combined_mask_dev, output_mask_dev = combined_mask_dev.to(device), output_mask_dev.to(device)\n",
    "                    y_pred_dev = model(X_dev, combined_mask_dev)\n",
    "                    loss_dev = compute_masked_loss(y_pred_dev, y_dev, output_mask_dev)\n",
    "                    precision_dev = compute_masked_precision(y_pred_dev, y_dev, output_mask_dev)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_id+1}/{len(train_loader)}, Loss: {loss.item()}, running loss: {running_loss}, Validation Loss: {loss_dev}, Validation Precision: {precision_dev}')\n",
    "\n",
    "            if loss_dev < lowest_loss:\n",
    "                lowest_loss = loss_dev\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                num_bad_batch = 0\n",
    "            else:\n",
    "                num_bad_batch += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if num_bad_epochs >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break   \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}, Validation Loss: {loss_dev}, Validation Precision: {precision_dev}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test set\n",
    "def inference(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (X_test, combined_mask_test, output_mask_test) in test_loader:\n",
    "            X_test = X_test.to(device)\n",
    "            combined_mask_test, output_mask_test = combined_mask_test.to(device), output_mask_test.to(device)\n",
    "            y_pred_test = model(X_test, combined_mask_test)\n",
    "            loss_test = compute_masked_loss(y_pred_test, X_test, output_mask_test)\n",
    "            precision_test = compute_masked_precision(y_pred_test, X_test, output_mask_test)\n",
    "\n",
    "    print(f'Test Loss: {loss_test}, Test Precision: {precision_test}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model for inference\n",
    "def get_derivative(string):\n",
    "    # convert string to list of tokens\n",
    "    tokens = add_space(string)\n",
    "    # convert tokens to tensor\n",
    "    input_seq = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # shape of input_seq: (1, seq_len)\n",
    "    # create positional encoding\n",
    "    pos_enc = pos_encodings(input_seq.shape[1], d_model).to(device)\n",
    "    # shape of pos_enc: (1, seq_len, d_model)\n",
    "\n",
    "\n",
    "    def inference(model, input_seq):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.tensor(vocab(input_seq), dtype=torch.long).unsqueeze(0).to(device)\n",
    "            preds = model(input_seq, None)[0]\n",
    "            next_token = preds.argmax(dim=-1).squeeze()\n",
    "        \n",
    "        output_seq = input_seq\n",
    "        if next_token==vocab['<eos>']:\n",
    "            # turn the output_seq to string\n",
    "            output_seq = [vocab.itos[i] for i in output_seq]\n",
    "            return output_seq   \n",
    "        else:\n",
    "            output_seq = torch.cat((output_seq, next_token), dim=1)\n",
    "            return inference(model, output_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
